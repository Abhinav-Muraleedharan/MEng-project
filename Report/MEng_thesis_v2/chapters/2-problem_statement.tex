% Note that depending on your settings in the table of contents, subsections and subsubsections might appear virtually identical.
% Make sure to set the ToC depth and the numbering depth in the ToC the way you want.
\chapter{Problem Statement}\label{ch:problem_statement}

Imagine we are recording data from $D$ neurons distributed across different regions of the brain. Let $x(t_i) \in \mathbb{R}^D$ denote the observed neural activity at timestep $t_i$ and let $y_i$ denote the observed behaviour of the animal at timestep $t_i$. From the time series dataset $\mathcal{D} = \{(x_i,y_i,t_i) \}_{i=1}^N$ of neural recordings, our goal is to
construct:
\begin{itemize}
    \item A predictive model of underlying brain dynamics
    \item A probabilistic model to predict behaviour of the organism at time $t+1$ given brain recordings until timestep $t$.
\end{itemize}
More formally, let's assume that the spiking activity is generated by an underlying non-stationary stochastic process defined by $p_t(x)$. 
\begin{equation}
    x(t) \sim p_t(x)
\end{equation}
The probability of observing a sequence of neural recordings and behavior can be expressed as:
\begin{equation}
    p( \{x_1,y_1\},\{x_2,y_2\},\{x_3,y_3\},..) = 
    \lim_{N \to \infty} \prod_{i=1}^{N} p(\{x_{i},y_{i}\}| \{x_1,y_1\},\{x_2,y_2\},..\{x_{i-1},y_{i-1}\})
\end{equation}
In the context of neural recordings, it is convenient to assume that the neural recording data and behavior can be modelled with separate probability distributions of the form:
\begin{equation}
   \prod_{i=1}^{N} p_d(\{x_{i}\}| \{x_1\},\{x_2\},..\{x_{i-1}\})
\end{equation}
\begin{equation}
   \prod_{i=1}^{N} p_b(\{y_{i}\}| \{x_1\},\{x_2\},..\{x_{i-1}\})
\end{equation}
Specifically, we assume that that the neural observed neural spiking data at timestep $t_i$ is not dependent on the behavior variables in the preceding timesteps. Probability distributions of this nature have been extensively investigated in the field of language modeling. In conventional autoregressive frameworks, the approximation of conditional distributions often involves the utilization of parameterized models constrained by a finite context limit \cite{vaswani2017attention}. While autoregressive models of this kind have been extremely successful in generating plausible language \cite{radford2018improving}, they still struggle to capture long-range dependencies due to the finite context length limit\cite{hahn2020theoretical}. Furthermore, the complexity of training and inference of transformer-based models is $\mathcal{O}(N^2)$, where $N$ is the context length of the transformer model.\\

